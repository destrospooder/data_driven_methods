[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ME493 - Methods of Data-Driven Control",
    "section": "",
    "text": "Preface\nDocumentation of the independent study ME493: Methods of Data-Driven Control, taken during Spring 2024.\nInstructor: Dirk M. Luchtenburg Co-conspirators: Sohaib Bhatti, Khushant Khurana, Eunkyu (Q) Kim, Sonam Okuda Email: dirk.luchtenburg@cooper.edu\nDescription: This independent study provides an introduction to state-of-the-art methods employed in the field of data-driven engineering. Data-driven methods such as the proper orthogonal decomposition and the dynamic mode decomposition are used in a variety of fields including fluid dynamics, climate analysis, mixing problems, the study of infectious diseases, etc. Data-driven methods rely heavily on concepts from linear algebra, calculus, probability, and statistics. We will review these essential elements and develop a hands-on understanding of a selection of data-driven methods. The study culminates with an application of some method(s) to a practical problem which is to be carefully detailed in a technical report.\nReferenced texts include the following:\n\nStrang (2019) \nBrunton and Kutz (2022) \nBoyd and Vandenberghe (2018) \nGéron (2022) \nSutton and Barto (2018)\n\n\n\n\n\nBoyd, Stephen, and Lieven Vandenberghe. 2018. Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares. Cambridge University Press.\n\n\nBrunton, Steven L, and J Nathan Kutz. 2022. Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control. Vol. 2. Cambridge University Press.\n\n\nGéron, Aurélien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow. O’Reilly Media, Inc.\n\n\nStrang, Gilbert. 2019. Linear Algebra and Learning from Data. SIAM.\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT Press."
  },
  {
    "objectID": "ch1.html",
    "href": "ch1.html",
    "title": "1  The Four Fundamental Subspaces",
    "section": "",
    "text": "In the Strang-ian view of linear algebra, an \\(m\\) by \\(n\\) matrix \\(\\textbf{A}\\) is associated with four fundamental subspaces - two of \\(\\mathbb{R}^m\\) and two of \\(\\mathbb{R}^n\\). It’s easiest to illustrate this using an example matrix:\n\\[ \\textbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} = \\begin{bmatrix}\n\\textbf{r}_1 \\\\\n\\textbf{r}_2\n\\end{bmatrix} = \\begin{bmatrix}\n\\textbf{c}_1 &\n\\textbf{c}_2 &\n\\textbf{c}_3 \\end{bmatrix} \\]\nwhere:\n\\[ \\textbf{r}_1 = \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix} \\quad \\textbf{r}_2 = \\begin{bmatrix} 4 & 5 & 6 \\end{bmatrix} \\]\n\\[ \\textbf{c}_1 = \\begin{bmatrix} 1 & 4 \\end{bmatrix}^T \\quad \\textbf{c}_2 = \\begin{bmatrix} 2 & 5 \\end{bmatrix}^T \\quad \\textbf{c}_3 = \\begin{bmatrix} 3 & 6 \\end{bmatrix}^T \\]\n\n\n\n\n\n\nColumn Space\n\n\n\nThe column space (or range) \\(R(\\textbf{A})\\) contains all linear combinations of the column vectors of \\(\\textbf{A}\\).\n\n\nIt only takes two linearly independent vectors to span \\(\\mathbb{R}^2\\), and we have three! Our column space is \\(\\mathbb{R}^2\\).\n\n\n\n\n\n\nRow Space\n\n\n\nThe row space \\(R(\\textbf{A}^T)\\) contains all linear combinations of the column vectors of \\(\\textbf{A}^T\\) (or equivalently, the row vectors of \\(A\\)).\n\n\nTransposed row vectors \\(\\textbf{r}_1^T\\) and \\(\\textbf{r}_2^T\\) span the following plane:\n\\[ \\left\\{ \\begin{bmatrix} x_1 + 4x_2 \\\\ 2x_1 + 5x_2 \\\\ 3x_1 + 6x_2 \\end{bmatrix} : \\textbf{x} \\in \\mathbb{R}^2 \\right\\} \\]\n\n\n\n\n\n\nNull Space\n\n\n\nThe null space \\(N(\\textbf{A})\\) contains all solutions \\(\\textbf{u}\\) to \\(\\textbf{Au} = \\textbf{0}\\).\n\n\nSolving this equation yields the nontrivial solution:\n\\[ \\textbf{u} = \\begin{bmatrix} 1 \\\\ -2 \\\\ 1 \\end{bmatrix} \\]\nOur null space is the span of \\(\\textbf{u}\\), or the following line:\n\\[ \\left\\{ \\begin{bmatrix} x \\\\ -2x \\\\ x \\end{bmatrix} : x \\in \\mathbb{R} \\right\\} \\]\n\n\n\n\n\n\nLeft Null Space\n\n\n\nThe left null space \\(N(\\textbf{A}^T)\\) contains all solutions \\(\\textbf{v}\\) to \\(\\textbf{A}^T \\textbf{v} = \\textbf{0}\\).\n\n\nThis equation has no nontrivial solutions, so the left null space is the zero subspace.\nThe “big picture of linear algebra,” as Gil Strang puts it, is that for an \\(m\\) by \\(n\\) matrix \\(\\textbf{A}\\):\n\nThe column space/range \\(R(\\textbf{A})\\) is perpendicular to the left null space \\(N(\\textbf{A}^T)\\) in \\(\\mathbb{R}^m\\)\nThe row space \\(R(\\textbf{A}^T)\\) is perpendicular to the null space \\(N(\\textbf{A})\\) in \\(\\mathbb{R}^n\\)\n\n\n\n\n\n\n\nRank\n\n\n\nThe rank \\(r\\) of a matrix \\(\\textbf{A}\\) is the number of independent rows/columns, i.e., the row space and column space/range have the same dimension \\(r\\).\n\n\n\nThe dimension of the null space \\(N(\\textbf{A})\\) is \\(n-r\\) and the dimension of the left null space \\(N(\\textbf{A}^T)\\) is \\(m-r\\)."
  },
  {
    "objectID": "ch2.html#overview",
    "href": "ch2.html#overview",
    "title": "2  Singular Value Decomposition",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nThe singular value decomposition (SVD) is a fundamental matrix factorization with numerous applications in data analysis and scientific computing. Mathematically, the SVD of an \\(m\\times n\\) matrix \\(\\textbf{X}\\) is a factorization of the form:\n\\[ \\textbf{X} = \\textbf{U} \\Sigma \\textbf{V}^* \\]\nwhere \\(\\textbf{U}\\) is an \\(m\\times m\\) orthogonal matrix, \\(\\Sigma\\) is an \\(m\\times n\\) diagonal matix with non-negative real numbers on the diagonal, and \\(\\textbf{V}\\) is an \\(n\\times n\\) orthogonal matrix.\nThe SVD is particularly useful for analyzing large, high-dimensional datasets that can be well-approximated by matrices of much lower rank. By extracting the dominant patterms in the data, the SVD enables efficient dimensionality reduction, noise removal, and data compression. It is the foundation of techniques like principal component analysis (PCA) and is widely applied in fields such as signal processing, machine learning, and image analysis.\nProper orthogonal decomposition (POD) modes are a set of orthogonal basis functions that optimally represent a given dataset in a least-squares sense. They are obtained by performing an SVD on a data matrix. POD modes form an orthonormal basis, meaning the modes are mutually orthogonal and have unit norm.\nThe modes are ranked by their energy content, with the first mode capturing the most energy and subsequent modes capturing progressively less energy.\nOne last thing- the pseudoinverse can be computed using the singular value decomposition of a matrix. If \\(\\textbf{X} = \\textbf{U} \\Sigma \\textbf{V}^*\\) is the SVD of \\(\\textbf{X}\\), then the pseudoinverse is given by:\n\\[\\text{X}^+ = \\textbf{V} \\Sigma^{-1} \\textbf{U}^*\\]"
  },
  {
    "objectID": "ch2.html#pod-analysis-of-low-reynolds-number-pitching-airfoil-dns",
    "href": "ch2.html#pod-analysis-of-low-reynolds-number-pitching-airfoil-dns",
    "title": "2  Singular Value Decomposition",
    "section": "2.2 POD Analysis of Low Reynolds Number Pitching Airfoil DNS",
    "text": "2.2 POD Analysis of Low Reynolds Number Pitching Airfoil DNS\nThe dataset analyzed contains direct numerical simulations (DNS) of two-dimensional stationary and pitching flat-plate airfoils at a Reynolds number of 100. The dataset includes time-resolved snapshots of the velocity field, lift and drag coefficients, and airfoil kinematics spanning 40-100 convective time units. The cases consist of a stationary airfoil and eight different pitching frequencies. This dataset is part of a database intended to aid in the conception, training, demonstration, evaluation, and comparison of reduced-complexity models for fluid mechanics, created by Aaron Towne and Scott Dawson.\nThe dataset also includes a MATLAB function that provides a simple implementation of Dynamic Mode Decomposition (DMD), a data-driven method we’ll get to later.\nTo analyze the DNS data using POD, I first extracted the velocity components from the provided snapshots. I then computed the mean-corrected snapshots by subtracting the mean of each snapshot and arranged them into a matrix \\(\\textbf{X}\\).\nNext, I performed an economy-sized SVD on \\(\\textbf{X}\\). The squared singular values are plotted here, as a function of the mode index.\n\n\n\nsquared_sv\n\n\nThis plot reveals how many individual POD basis vectors there are. The rapid decay of the singular values indicates that the flow is well-approximated by a low-rank subspace, with the first 16 modes capturing the majority of the energy.\n\n\n\nsquared_sv_truncated\n\n\nVisualizing the first six POD modes for both \\(u_x\\) and \\(u_y\\) revealed the spatial structure of the dominant flow patterns. The oscillatory modes have a characteristic wavelength that can be estimated from the spatial distribution of the mode amplitudes.\n\n\n\nux_spatial_modes\n\n\n\n\n\nuy_spatial_modes\n\n\nHere are the temporal amplitudes associated with those spatial modes:\n\n\n\ntemporal_amplitudes\n\n\nFinally, I reconstructed the snapshots using a rank-4 approximation, which captures the most energetic flow structures. Comparing the reconstructed snapshots with the original data showed that the low-rank approximation successfully recovers the essential flow physics, such as the presence of coherent structures and the overall flow patterns.\n\n\n\nux_reconstruction\n\n\n\n\n\nuy_reconstruction"
  },
  {
    "objectID": "ch3.html#overview-dynamic-mode-decomposition",
    "href": "ch3.html#overview-dynamic-mode-decomposition",
    "title": "3  Data-Driven Dynamical Systems",
    "section": "3.1 Overview: Dynamic Mode Decomposition",
    "text": "3.1 Overview: Dynamic Mode Decomposition\nDynamic mode decomposition (DMD) is a dimensionality reduction technique developed in the fluid dynamics community to extract spatiotemporal coherent structures from high-dimensional data. Unlike principal component analysis (PCA) or proper orthogonal decomposition (POD), which focus solely on spatial correlations or energy content, DMD provides a modal decomposition where each mode is associated with a specific oscillation frequency and growth/decay rate. This allows DMD to capture the temporal evolution of the system, in addition to reducing the dimensionality. DMD achieves this by approximating the Koopman operator, which describes the linear dynamics governing the system, rather than just identifying the dominant spatial patterns like PCA/POD. As a result, DMD modes can be more physically meaningful than the orthogonal modes generated by PCA, as they directly correspond to the intrinsic temporal behaviors of the system.\nDMD operates on a series of data snapshots, typically obtained from simulations or experiments. These snapshots are organized into a matrix \\(\\textbf{V}\\), where:\n\\[ \\textbf{V} = \\left[ \\textbf{v}_1 \\quad \\textbf{v}_2 \\quad \\cdots \\quad \\textbf{v}_N \\right] \\in \\mathbb{R}^{M \\times N}\\]\nEach \\(\\textbf{v}_i \\in \\mathbb{R}^M\\) represents a snapshot at a specific time. DMD attempts to find a linear operator \\(\\textbf{A}\\) that approximates the evolution of the system from one snapshot to the next.\nFirst, the snapshot data is arranged into two matrices \\(\\textbf{X}\\) and \\(\\textbf{X}'\\), where \\(\\textbf{X}\\) contains the first N-1 snapshots and \\(\\textbf{X}'\\) contains the last N-1 snapshots.\nThe SVD of \\(\\textbf{X}\\) is taken. To deal with high-dimensional data, a reduced order representation may be used, by truncating the SVD to the first \\(r\\) modes.\n\\[ \\textbf{X} = \\textbf{U} \\Sigma \\textbf{V}^* \\approx \\textbf{U}_r \\Sigma_r \\textbf{V}^*_r\\]\nThe full matrix \\(\\textbf{A}\\) is found by computing the pseudo-inverse of \\(\\textbf{X}\\):\n\\[ \\textbf{A} = \\textbf{X}' \\textbf{V}_r \\Sigma_r^{-1} \\textbf{U}_r^* \\]\nThe next step involves computing the matrix \\(\\tilde{\\textbf{A}}\\), which is the projection of \\(\\textbf{A}\\) onto the subspace spanned by the columns of \\(\\textbf{U}_r\\).\n\\[ \\tilde{\\textbf{A}} = \\textbf{U}_r^* \\textbf{A} \\textbf{U}_r = \\textbf{U}_r^* \\textbf{X}' \\textbf{V}_r \\Sigma_r^{-1} \\]\nThen, we “eigendecompose” (is that a word?) \\(\\tilde{\\textbf{A}}\\) to find its eigenvalues \\(\\Lambda\\) and eigenvectors \\(\\textbf{W}\\). These eigenvalues and eigenvectors are used to compute the DMD modes \\(\\Phi\\), which are given by:\n\\[\\Phi = \\textbf{X}' \\textbf{V}_r \\Sigma_r^{-1} \\textbf{W}\\]\nNotably, these high-dimensional DMD modes are also the eigenvectors of the full \\(\\textbf{A}\\) matrix corresponding to the eigenvalues in \\(\\Lambda\\)."
  },
  {
    "objectID": "ch3.html#dmd-analysis-of-low-reynolds-number-pitching-airfoil-dns",
    "href": "ch3.html#dmd-analysis-of-low-reynolds-number-pitching-airfoil-dns",
    "title": "3  Data-Driven Dynamical Systems",
    "section": "3.2 DMD Analysis of Low Reynolds Number Pitching Airfoil DNS",
    "text": "3.2 DMD Analysis of Low Reynolds Number Pitching Airfoil DNS\nWe now return to the dataset analyzed using POD, but now we use the dynamic mode decomposition to analyze this data.\nThe modal amplitudes, and first six \\(u_x\\) and \\(u_y\\) modes are displayed below. It is clearly apparent that the modes are not identical to the POD modes. Due to the non-orthogonal nature of its modes, DMD-based representations can be less concise compared to those generated by PCA. However, DMD modes often provide greater physical insight, as each mode corresponds to a damped or driven sinusoidal time behavior.\n  \nHere, the spatial modes reveal the dominant patterns or structures within the flow. Each spatial mode corresponds to a specific frequency and growth/decay rate, providing insights into how different flow features contribute to the overall dynamics and how they change spatially across the domain."
  },
  {
    "objectID": "ch3.html#overview-sparse-identification-of-nonlinear-dynamics",
    "href": "ch3.html#overview-sparse-identification-of-nonlinear-dynamics",
    "title": "3  Data-Driven Dynamical Systems",
    "section": "3.3 Overview: Sparse Identification of Nonlinear Dynamics",
    "text": "3.3 Overview: Sparse Identification of Nonlinear Dynamics\nThe SINDy (Sparse Identification of Nonlinear Dynamics) algorithm is a technique designed to uncover the governing equations of a dynamical system from observed data. It works by identifying a sparse (or concise) representation of the system’s dynamics, focusing on the most relevant terms that describe the system’s behavior.\nThe process begins with collecting time-series data of the state variables of the system. From this data, a comprehensive library of candidate functions, which may include polynomials, trigonometric functions, and other nonlinear terms, is constructed. These functions represent potential components of the system’s underlying equations.\nSINDy then employs sparse regression techniques to sift through this library, selecting only the most pertinent functions. The goal is to find a minimal set of terms that can accurately describe the dynamics, resulting in a sparse representation of the system’s governing equations. This approach ensures that the resulting model is both interpretable and parsimonious, capturing the essential dynamics without unnecessary complexity."
  },
  {
    "objectID": "ch3.html#sindy-implementation-on-temporal-amplitudes",
    "href": "ch3.html#sindy-implementation-on-temporal-amplitudes",
    "title": "3  Data-Driven Dynamical Systems",
    "section": "3.4 SINDy Implementation on Temporal Amplitudes",
    "text": "3.4 SINDy Implementation on Temporal Amplitudes\nWe attempt to fit a dynamic system \\(\\dot{\\textbf{x}} = f(\\textbf{x})\\) to the POD temporal amplitudes using the SINDy algorithm. We select a number of temporal amplitudes to consider (6, in this case) and construct the matrix of temporal amplitudes tempamps by scaling the right singular vectors \\(\\textbf{V}\\) with the singular values \\(\\Sigma\\).\nNext, we compute the time derivatives of the temporal amplitudes using finite differences. We then pool the data to form a library of candidate functions \\(\\Theta\\), including polynomial terms up to the second order, using a function that constructs a matrix where each column is a candidate term for the dynamics, such as constant, linear, and quadratic terms of the state variables.\nWe perform sequential thresholding least squares to find a sparse matrix \\(\\Xi\\) that best fits the time derivatives \\(\\dot{\\textbf{x}}\\) to the candidate functions in \\(\\Theta\\). The sparsification process involves iteratively zeroing out small coefficients (below the threshold value) and refitting the remaining coefficients.\nThe system is then integrated using cumulative trapezoidal numerical integration. This was initially attempted using Runge-Kutta methods, but these methods proved ineffective likely because the system is multi-scale (and rather complicated), leading to instability and inaccuracies in the solutions.\nWe then compare these SINDy-derived amplitudes with the original POD temporal amplitudes by plotting them. The plots show the effectiveness of the SINDy model in capturing the dynamics.\n\n\n\nsindy_fit\n\n\nUsage of the PySINDy library was also experimented with - it yielded identical results."
  },
  {
    "objectID": "ch4.html#overview-eigensystem-realization-algorithm",
    "href": "ch4.html#overview-eigensystem-realization-algorithm",
    "title": "4  System Identification Techniques",
    "section": "4.1 Overview: Eigensystem Realization Algorithm",
    "text": "4.1 Overview: Eigensystem Realization Algorithm\nThe Eigensystem Realization Algorithm (ERA) is a method used in system identification to derive a state-space model from time-domain data, typically impulse or step response data.\n\n\n\n\n\n\nHankel Matrix\n\n\n\nA Hankel matrix is a structured matrix where each ascending diagonal from left to right is constant.\n\n\nTwo Hankel matrices are constructed from the collected data. Let \\(\\textbf{y}\\) be the sequence of observed data points. Then the Hankel matrix \\(\\textbf{H}_0\\) and the shifted Hankel matrix \\(\\textbf{H}_1\\) are defined as:\n\\[\n\\textbf{H}_0 = \\begin{bmatrix}\ny_1 & y_2 & \\cdots & y_m \\\\\ny_2 & y_3 & \\cdots & y_{m+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_n & y_{n+1} & \\cdots & y_{n+m-1}\n\\end{bmatrix}\n\\]\n\\[\n\\textbf{H}_1 = \\begin{bmatrix}\ny_2 & y_3 & \\cdots & y_{m+1} \\\\\ny_3 & y_4 & \\cdots & y_{m+2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_{n+1} & y_{n+2} & \\cdots & y_{n+m}\n\\end{bmatrix}\n\\]\nThe singular value decomposition (SVD) is performed on \\(\\textbf{H}_0\\) (and truncated to keep only the most significant singular values):\n\\[\\textbf{H}_0 = \\textbf{U} \\Sigma \\textbf{V}^* \\approx \\textbf{U}_r \\Sigma_r \\textbf{V}^*_r\\]\nThe reduced matrices are used to construct the state-space matrices \\(\\textbf{A}\\), \\(\\textbf{B}\\), and \\(\\textbf{C}\\):\n\\[\\textbf{A} = \\Sigma_r^{-0.5} \\textbf{U}_r^* \\textbf{H}_1 \\textbf{V}_r \\Sigma_r^{-0.5}\\] \\[\\textbf{B} = \\Sigma_r^{0.5} \\textbf{V}_r^* [:, 0]\\] \\[\\textbf{C} = \\textbf{U}_r [0, :] \\Sigma_r^{0.5} \\]\nWhen constructing the Hankel matrices, collecting sufficient data ensures balanced controllability and observability Gramians \\(\\mathcal{O}_d \\mathcal{O}_d^*\\) and \\(\\mathcal{C}_d \\mathcal{C}_d^*\\). If data is insufficient, ERA only approximates the balance. Alternatively, collecting enough data for the Hankel matrices to reach numerical full rank (where remaining singular values are below a threshold) yields a low-order ERA model."
  },
  {
    "objectID": "ch4.html#atomic-force-microscope-transfer-function-recovery",
    "href": "ch4.html#atomic-force-microscope-transfer-function-recovery",
    "title": "4  System Identification Techniques",
    "section": "4.2 Atomic Force Microscope Transfer Function Recovery",
    "text": "4.2 Atomic Force Microscope Transfer Function Recovery\nWe use the Eigensystem Realization Algorithm (ERA) to perform system identification on a transfer function representing the dynamics of an atomic force microscope.\n\\[ G(s) = \\frac{k\\omega_2^2 \\omega_3^2 \\omega_5^2 \\left( s^2 + 2\\zeta_1 \\omega_1 s + \\omega_1^2 \\right) \\left( s^2 + 2\\zeta_4 \\omega_4 s + \\omega_4^2 \\right) e^{-s\\tau}}{\\omega_1^2 \\omega_4^2 \\left( s^2 + 2\\zeta_2 \\omega_2 s + \\omega_2^2 \\right) \\left( s^2 + 2\\zeta_3 \\omega_3 s + \\omega_3^2 \\right) \\left( s^2 + 2\\zeta_5 \\omega_5 s + \\omega_5^2 \\right)} \\]\nwith \\(\\omega_i = 2\\pi f_i\\), \\(k\\) = 5,\n\\[\\begin{align*}\nf_1 &= 2.4 \\text{ kHz}, & f_2 &= 2.6 \\text{ kHz}, & f_3 &= 6.5 \\text{ kHz}, & f_4 &= 8.3 \\text{ kHz}, & f_5 &= 9.3 \\text{ kHz,} \\\\\n\\zeta_1 &= 0.03, & \\zeta_2 &= 0.03, & \\zeta_3 &= 0.042, & \\zeta_4 &= 0.025, & \\zeta_5 &= 0.032\n\\end{align*}\\]\nand \\(\\tau = 10^{-4}\\) seconds.\nA Padé approximant is used to handle the time delay, as the control library does not support time delays directly.\nThe impulse response of the system is computed over a specified time range.\n\n\n\nImpulse response of the provided transfer function.\n\n\nHankel matrices are constructed using the impulse response data. The SVD is used to decompose the Hankel matrix into components that facilitate the identification of the system’s state-space representation.\nThe state-space matrices \\(\\textbf{A}\\), \\(\\textbf{B}\\), and \\(\\textbf{C}\\) are calculated using the truncated SVD components and fractional matrix powers. This step essentially reduces the system to a simplified model while preserving its significant dynamics.\nThe squared singular values of the Hankel matrix are plotted to identify the most significant components.\n\n\n\nMost significant Hankel squared singular values.\n\n\nA new state-space model is created using the ERA results. Bode plots of the original and reconstructed systems are generated and compared. These plots visualize the frequency response of both systems, ensuring that the reconstructed model accurately represents the original system’s dynamics.\n\n\n\nFrequency responses of the original and reconstructed images.\n\n\nDue to the use of the Padé approximant for handling the time delay in the system, there are minor discrepancies in the phase of the frequency response. While it effectively captures the overall behavior of the delay, it can introduce slight inaccuracies in the phase response, especially at higher frequencies. Consequently, the phase response of the system might not perfectly align with the actual phase characteristics, although the magnitude response remains largely unaffected."
  },
  {
    "objectID": "ch4.html#overview-dmd-with-control",
    "href": "ch4.html#overview-dmd-with-control",
    "title": "4  System Identification Techniques",
    "section": "4.3 Overview: DMD with Control",
    "text": "4.3 Overview: DMD with Control\nDynamic Mode Decomposition with control (DMDc) is a modification of the standard DMD algorithm designed to handle input-output systems where actuation or control inputs are present. In short, the DMDc method tries to find the best-fit linear operators \\(\\textbf{A}\\) and \\(\\textbf{B}\\) that approximately describe the following dynamics based on measurement data:\n\\[\\textbf{x}_{k+1} \\approx \\textbf{A} \\textbf{x}_k + \\textbf{B} \\textbf{u}_k\\]\n\\(\\textbf{X}\\) and \\(\\textbf{X}\\)’ are defined as they were for standard DMD. A matrix of the actuation input history is assembled, defined as follows:\n\\[\\Upsilon = \\begin{bmatrix} \\textbf{u}_1 & \\textbf{u}_2 & \\cdots & \\textbf{u}_N \\end{bmatrix}\\]\n(\\(\\Upsilon\\) is used in lieu of \\(\\textbf{U}\\) to disambiguate between the \\(\\textbf{U}\\) matrix in the SVD.) The dynamics are now written:\n\\[ \\textbf{X}' \\approx \\begin{bmatrix} \\textbf{A} & \\textbf{B} \\end{bmatrix} \\begin{bmatrix} \\textbf{X} \\\\ \\Upsilon \\end{bmatrix} = \\textbf{G} \\Omega\\]\n\\(\\textbf{G} = \\begin{bmatrix} \\textbf{A} & \\textbf{B} \\end{bmatrix}\\) can be isolated using least-squares regression:\n\\[\\textbf{G} \\approx \\textbf{X}' \\Omega^+\\]\nThe singular value decomposition of \\(\\Omega = \\begin{bmatrix} \\textbf{X}^* & \\Upsilon^* \\end{bmatrix}^*\\) is taken:\n\\[\\Omega = \\tilde{\\textbf{U}} \\tilde{\\Sigma} \\tilde{\\textbf{V}}^*\\]\n\\(\\tilde{\\textbf{U}}\\) is split into two matrices to provide bases for \\(\\textbf{X}\\) and \\(\\Upsilon\\):\n\\[\\tilde{\\textbf{U}} = \\begin{bmatrix} \\tilde{\\textbf{U}}_1^* & \\tilde{\\textbf{U}}_2^* \\end{bmatrix}\\]\nThe state matrices \\(\\textbf{A}\\) and \\(\\textbf{B}\\) can now be constructed:\n\\[\\textbf{A} = \\textbf{X}' \\tilde{\\textbf{V}} \\tilde{\\Sigma}^{-1} \\tilde{\\textbf{U}}_1^*\\] \\[\\textbf{B} = \\textbf{X}' \\tilde{\\textbf{V}} \\tilde{\\Sigma}^{-1} \\tilde{\\textbf{U}}_2^*\\]\n\\(\\tilde{\\textbf{U}}\\) provides a reduced basis for the input space, whereas \\(\\hat{\\textbf{U}}\\) from:\n\\[\\textbf{X}' = \\hat{\\textbf{U}} \\hat{\\Sigma} \\hat{\\textbf{V}}^*\\]\nprovides a reduced basis for the output space. These bases allow us to reduce the order of \\(\\textbf{G}\\) by projecting onto this basis:\n\\[\\tilde{\\textbf{G}} = \\hat{\\textbf{U}}^* \\textbf{G} \\begin{bmatrix} \\hat{\\textbf{U}} \\\\ \\textbf{I} \\end{bmatrix}\\]\nand the corresponding projected matrices \\(\\tilde{\\textbf{A}}\\) and \\(\\tilde{\\textbf{B}}\\) are:\n\\[\\tilde{\\textbf{A}} = \\hat{\\textbf{U}}^* \\textbf{A}\\hat{\\textbf{U}}\\]\n\\[\\tilde{\\textbf{B}} = \\hat{\\textbf{U}}^* \\textbf{B} \\]\nThe key difference from standard DMD is the augmented data matrix \\(\\textbf{G}\\), which incorporates both state and control input data. This allows DMDc to disambiguate the underlying dynamics from the effects of actuation, providing an accurate input-output model represented by the modes \\(\\Phi\\) and eigenvalues \\(\\Lambda\\).\n\\[\\tilde{\\textbf{A}} \\textbf{W} = \\textbf{W} \\Lambda\\]\n\\[ \\Phi = \\textbf{X}' \\tilde{\\textbf{V}} \\tilde{\\Sigma}^{-1} \\tilde{\\textbf{U}}_1^* \\hat{\\textbf{U}} \\textbf{W} \\]"
  },
  {
    "objectID": "ch5.html",
    "href": "ch5.html",
    "title": "5  2024SP Project: Population Dynamics",
    "section": "",
    "text": "For my final project, I implemented the SINDyC algorithm to identify the governing eqquations of a modified Lotka-Volterra predator-prey model with an external forcing term that artificially inflates the prey population. I then implemented a Model Predictive Control (MPC) strategy to control the system, using the equations identified by the SINDy algorithm.\n\\[\n\\dot{x}_1 = \\alpha x_1 - \\beta x_1 x_2 \\\\\\] \\[\n\\dot{x}_2 = -\\gamma x_2 + \\delta x_1 x_2 + F(t)\n\\]\nwhere \\(\\alpha\\) represents the max prey per capita growth rate, \\(\\beta\\) represents the effect of the presence of predators on the prey death rate, \\(\\gamma\\) represents the predator per capita death rate, and \\(\\delta\\) represents the effect of the presence of prey on predator birth rate.\nTime-series data for the prey and predator population are generated by numerically solving the equations for a specified input. Options are provided to add Gaussian noise to the data and visualize the noisy and noiseles time-series.\nNext, the SINDyC algorithm is set up using the PySINDy library. The algorithm is configured with a finite difference differentiation method, a polunomial feature library of degree 2, and a sequential thresholded least-squares optimizer. The governing equations are represented as sparse coefficients for the candidate library functions.\nThe MPC controller is configured using the do_mpc library. The state variables are the prey and predator populations, and the control input is an unspecified external action.\nThe objective function for the MPC controller is defined as minimizing the sum of squared population rates of prey and predators, effectively aiming to stabilize the populations of both species. The MPC controller and a simulator are initialized with the same initial conditions for the prey and predator populations. A simulation loop is run for 20 time steps, where the MPC controller computes the optimal control input at each step, and the simulator updates the system state based on the control input.\nFigures can be found on my repository."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Boyd, Stephen, and Lieven Vandenberghe. 2018. Introduction to\nApplied Linear Algebra: Vectors, Matrices, and Least Squares.\nCambridge University Press.\n\n\nBrunton, Steven L, and J Nathan Kutz. 2022. Data-Driven Science and\nEngineering: Machine Learning, Dynamical Systems, and Control. Vol.\n2. Cambridge University Press.\n\n\nGéron, Aurélien. 2022. Hands-on Machine Learning with Scikit-Learn,\nKeras, and TensorFlow. O’Reilly Media, Inc.\n\n\nStrang, Gilbert. 2019. Linear Algebra and Learning from Data.\nSIAM.\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning:\nAn Introduction. MIT Press."
  }
]